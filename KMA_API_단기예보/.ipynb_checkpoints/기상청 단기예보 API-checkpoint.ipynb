{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac733d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# 로깅 설정\n",
    "log_folder = 'logs'\n",
    "if not os.path.exists(log_folder):\n",
    "    os.makedirs(log_folder)\n",
    "\n",
    "log_file_path = os.path.join(log_folder, 'weather_data_fetch.log')\n",
    "logging.basicConfig(filename=log_file_path, format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "class WeatherDataFetcher:\n",
    "    def __init__(self, api_key_file):\n",
    "        self.api_key = self.read_api_key(api_key_file)\n",
    "        if not self.api_key:\n",
    "            logging.error(\"API key reading failed. Exiting.\")\n",
    "            exit(1)\n",
    "        self.api_url = \"https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_api_key(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                return f.read().strip()\n",
    "        except FileNotFoundError:\n",
    "            logging.error(\"API key file not found.\")\n",
    "            return None\n",
    "\n",
    "    def fetch_all_weather_data(self, params):\n",
    "        # 요청 URL을 생성합니다.\n",
    "        request_url = requests.Request('GET', self.api_url, params=params).prepare().url\n",
    "        print(f\"Request URL: {request_url}\")  # 생성된 URL을 출력합니다.\n",
    "\n",
    "        try:\n",
    "            res = requests.get(self.api_url, params=params)\n",
    "            res.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            logging.error(f\"API request failed for params {params}: {e}\")\n",
    "            return None\n",
    "    \n",
    "        root = ET.fromstring(res.text)\n",
    "        item_list = root.findall('./body/items/item')\n",
    "        if not item_list:\n",
    "            logging.warning(f\"No items found in the XML response for params {params}.\")\n",
    "            return None\n",
    "\n",
    "        result_list = []\n",
    "        for item in item_list:\n",
    "            item_dict = {}\n",
    "            for elem in item:\n",
    "                tag = elem.tag\n",
    "                value = elem.text or \"Nan\"\n",
    "                item_dict[tag] = value\n",
    "            result_list.append(item_dict)\n",
    "\n",
    "        return result_list\n",
    "\n",
    "    def fetch_data_for_location_and_time(self, base_date, base_time, nx, ny, custom_file_name=None, num_rows=809, page_range=1):\n",
    "        params = {\n",
    "            'serviceKey': self.api_key,\n",
    "            'pageNo': 1,\n",
    "            'numOfRows': num_rows,\n",
    "            'dataType': 'XML',\n",
    "            'base_date': base_date,\n",
    "            'base_time': base_time,\n",
    "            'nx': nx,\n",
    "            'ny': ny\n",
    "        }\n",
    "\n",
    "        result_dicts = []\n",
    "        for i in range(1, page_range + 1):\n",
    "            params['pageNo'] = i\n",
    "            result_list = self.fetch_all_weather_data(params)\n",
    "            if result_list:\n",
    "                result_dicts.extend(result_list)\n",
    "\n",
    "        df = pd.DataFrame(result_dicts)\n",
    "        if df.empty:\n",
    "            logging.warning(f\"No data to save for params {params}.\")\n",
    "            return None\n",
    "\n",
    "        grouped = df.groupby(['baseDate', 'baseTime', 'fcstDate', 'fcstTime', 'nx', 'ny'])\n",
    "        new_df = pd.DataFrame()\n",
    "\n",
    "        for name, group in grouped:\n",
    "            record = {\n",
    "                'baseDate': name[0],\n",
    "                'baseTime': name[1],\n",
    "                'fcstDate': name[2],\n",
    "                'fcstTime': name[3],\n",
    "                'nx': name[4],\n",
    "                'ny': name[5]\n",
    "            }\n",
    "            for i, row in group.iterrows():\n",
    "                category = row['category']\n",
    "                value = row['fcstValue']\n",
    "                record[category] = value\n",
    "\n",
    "            new_df = pd.concat([new_df, pd.DataFrame([record])], ignore_index=True)\n",
    "        \n",
    "        data_folder = 'data'\n",
    "        if not os.path.exists(data_folder):\n",
    "            os.makedirs(data_folder)\n",
    "            \n",
    "        if custom_file_name:\n",
    "            csv_file_path = os.path.join(data_folder, custom_file_name)\n",
    "        else:\n",
    "            csv_file_name = f'weather_data_{base_date}_{base_time}_{nx}_{ny}.csv'\n",
    "            csv_file_path = os.path.join(data_folder, csv_file_name)\n",
    "        new_df.to_csv(csv_file_path, index=False, encoding='cp949')\n",
    "        logging.info(f\"Data has been saved to {csv_file_path}\")\n",
    "        return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98b5e17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request URL: https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst?serviceKey=SGD3J7nUrX%2Bod%2B6SN3Fu%2BZB4N9DS13PXL4NV7jwYaFGTFjJSmN4Uk2eIr3ErirViBOIzt2L%2FiYtsg%2BItaPW6YA%3D%3D&pageNo=1&numOfRows=809&dataType=XML&base_date=20240221&base_time=0500&nx=52&ny=79\n",
      "Request URL: https://apis.data.go.kr/1360000/VilageFcstInfoService_2.0/getVilageFcst?serviceKey=SGD3J7nUrX%2Bod%2B6SN3Fu%2BZB4N9DS13PXL4NV7jwYaFGTFjJSmN4Uk2eIr3ErirViBOIzt2L%2FiYtsg%2BItaPW6YA%3D%3D&pageNo=1&numOfRows=809&dataType=XML&base_date=20240221&base_time=0500&nx=59&ny=74\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "#from WeatherDataFetcher import WeatherDataFetcher  # WeatherDataFetcher 클래스를 정의한 모듈을 불러옵니다.\n",
    "\n",
    "# API 키 파일과 날짜, 시간 설정\n",
    "api_key_file = \"KMA_API_KEY(Decoding).txt\"\n",
    "base_date = datetime.datetime.now().strftime('%Y%m%d')  # 시스템의 현재 날짜를 YYYYMMDD 형식으로 가져옵니다.\n",
    "base_time = '0500'  # 고정 시간\n",
    "\n",
    "# WeatherDataFetcher 인스턴스 생성\n",
    "fetcher = WeatherDataFetcher(api_key_file)\n",
    "\n",
    "# 격자 좌표 파일 불러오기\n",
    "grid_points_path = 'API_grid_points.csv'  # 실제 파일 경로로 변경해야 합니다.\n",
    "grid_points_df = pd.read_csv(grid_points_path, encoding='cp949')\n",
    "\n",
    "# 각 격자 좌표에 대해 API 호출\n",
    "for index, row in grid_points_df.iterrows():\n",
    "    nx = row['nx']\n",
    "    ny = row['ny']\n",
    "    fetcher.fetch_data_for_location_and_time(base_date, base_time, nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e450417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 데이터 폴더 내의 모든 파일을 순회하며 분석\n",
    "data_folder = './data'\n",
    "for file_name in os.listdir(data_folder):\n",
    "    if file_name.endswith('.csv'):\n",
    "        weather_data_path = os.path.join(data_folder, file_name)\n",
    "        weather_df = pd.read_csv(weather_data_path, encoding='cp949')\n",
    "\n",
    "        # nx, ny 좌표 추출\n",
    "        nx, ny = file_name.split('_')[-2], file_name.split('_')[-1].replace('.csv', '')\n",
    "\n",
    "        # 체감온도 계산 함수\n",
    "        def calculate_heat_index(temp, humidity):\n",
    "            return temp + (humidity / 10)\n",
    "\n",
    "        # 폭염과 강풍 분석 함수\n",
    "        def analyze_weather_conditions(df):\n",
    "            heat_indexes = df.apply(lambda x: calculate_heat_index(x['TMX'], x['REH']), axis=1)\n",
    "            heat_wave = heat_indexes.max() >= 33\n",
    "            strong_wind = df['WSD'].max() > 50.4\n",
    "            return heat_wave, strong_wind\n",
    "\n",
    "        # 날짜별 분석 함수\n",
    "        def analyze_daily_weather(df, base_date):\n",
    "            daily_report = \"\"\n",
    "            for date in sorted(df['fcstDate'].unique()):\n",
    "                if date > base_date + 2:\n",
    "                    continue\n",
    "                daily_df = df[df['fcstDate'] == date]\n",
    "                max_temp = daily_df['TMX'].max()\n",
    "                min_temp = daily_df['TMN'].min() if not pd.isna(daily_df['TMN'].min()) else daily_df['TMP'].min()\n",
    "                max_precipitation = daily_df['PCP'].max()\n",
    "                max_snowfall = daily_df['SNO'].max()\n",
    "\n",
    "                daily_report += f\"날짜: {date}({nx}, {ny})\\n\"\n",
    "                daily_report += f\"최고 기온: {max_temp}°C\\n\"\n",
    "                daily_report += f\"최저 기온: {min_temp}°C\\n\"\n",
    "                daily_report += f\"최대 강수량: {max_precipitation}\\n\"\n",
    "                daily_report += f\"최대 적설량: {max_snowfall}\\n\\n\"\n",
    "            return daily_report\n",
    "\n",
    "        # 기준 날짜 설정\n",
    "        base_date = min(weather_df['fcstDate'].unique())\n",
    "\n",
    "        # 폭염과 강풍 분석\n",
    "        heat_wave, strong_wind = analyze_weather_conditions(weather_df)\n",
    "\n",
    "        # 날짜별 날씨 분석\n",
    "        daily_weather_report = analyze_daily_weather(weather_df, base_date)\n",
    "\n",
    "        # 최종 보고서 출력\n",
    "        report = f\"오늘의 날씨 분석 보고서({nx}, {ny}):\\n\"\n",
    "        report += f\"폭염 여부: {'있음' if heat_wave else '없음'}\\n\"\n",
    "        report += f\"강풍 여부: {'있음' if strong_wind else '없음'}\\n\\n\"\n",
    "        report += daily_weather_report\n",
    "        report += \"※ 참고사항: 보고서 내용은 기상청 예보 발표 시간 5:00am 이후 수치에 기반하여 작성되었습니다.\"\n",
    "\n",
    "        # 보고서를 파일로 저장\n",
    "        report_file_name = f'weather_report_{nx}_{ny}.txt'\n",
    "        report_file_path = os.path.join(data_folder, report_file_name)\n",
    "        with open(report_file_path, 'w', encoding='utf-8') as report_file:\n",
    "            report_file.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9f61fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 지역명 매핑을 위한 데이터를 로드합니다.\n",
    "region_mapping_path = './region_grid_data_sigungu.csv'  # 실제 파일 경로로 수정해야 합니다.\n",
    "region_mapping_df = pd.read_csv(region_mapping_path, encoding='cp949')\n",
    "\n",
    "# 지역명 매핑을 위한 딕셔너리 생성\n",
    "region_dict = {f\"{row['nx']}_{row['ny']}\": row['지역명'] for _, row in region_mapping_df.iterrows()}\n",
    "\n",
    "# 데이터 폴더 내의 모든 파일을 순회하며 분석\n",
    "data_folder = './data'  # 실제 데이터 폴더 경로로 수정해야 합니다.\n",
    "for file_name in os.listdir(data_folder):\n",
    "    if file_name.endswith('.csv'):\n",
    "        # nx, ny 좌표 추출 및 지역명 매핑\n",
    "        nx, ny = file_name.split('_')[-2], file_name.split('_')[-1].replace('.csv', '')\n",
    "        region_name = region_dict.get(f\"{nx}_{ny}\", \"알 수 없는 지역\")\n",
    "\n",
    "        weather_data_path = os.path.join(data_folder, file_name)\n",
    "        weather_df = pd.read_csv(weather_data_path, encoding='cp949')\n",
    "\n",
    "\n",
    "        # 체감온도 계산 함수\n",
    "        def calculate_heat_index(temp, humidity):\n",
    "            return temp + (humidity / 10)\n",
    "\n",
    "        # 폭염과 강풍 분석 함수\n",
    "        def analyze_weather_conditions(df):\n",
    "            heat_indexes = df.apply(lambda x: calculate_heat_index(x['TMX'], x['REH']), axis=1)\n",
    "            heat_wave = heat_indexes.max() >= 33\n",
    "            strong_wind = df['WSD'].max() > 50.4\n",
    "            return heat_wave, strong_wind\n",
    "\n",
    "        # 날짜별 분석 함수\n",
    "        def analyze_daily_weather(df, base_date):\n",
    "            daily_report = \"\"\n",
    "            for date in sorted(df['fcstDate'].unique()):\n",
    "                if date > base_date + 2:\n",
    "                    continue\n",
    "                daily_df = df[df['fcstDate'] == date]\n",
    "                print(daily_df)\n",
    "                max_temp = daily_df['TMX'].max()\n",
    "                min_temp = daily_df['TMN'].min() if not pd.isna(daily_df['TMN'].min()) else daily_df['TMP'].min()\n",
    "                max_precipitation = daily_df['PCP'].max()\n",
    "                max_snowfall = daily_df['SNO'].max()\n",
    "\n",
    "                daily_report += f\"날짜: {date}\\n\"\n",
    "                daily_report += f\"최고 기온: {max_temp}°C\\n\"\n",
    "                daily_report += f\"최저 기온: {min_temp}°C\\n\"\n",
    "                daily_report += f\"최대 강수량: {max_precipitation}\\n\"\n",
    "                daily_report += f\"최대 적설량: {max_snowfall}\\n\\n\"\n",
    "            return daily_report\n",
    "        \n",
    "        # 최종 보고서 출력\n",
    "        report = f\"오늘의 날씨 분석 보고서:\\n\"\n",
    "        report += f\"폭염 여부: {'있음' if heat_wave else '없음'}\\n\"\n",
    "        report += f\"강풍 여부: {'있음' if strong_wind else '없음'}\\n\\n\"\n",
    "        report += daily_weather_report\n",
    "        report += \"※ 참고사항: 보고서 내용은 기상청 예보 발표 시간 5:00am 이후 수치에 기반하여 작성되었습니다.\"\n",
    "\n",
    "        # 보고서를 파일로 저장\n",
    "        report_file_name = f'weather_report_{region_name}.txt'\n",
    "        report_file_path = os.path.join(data_folder+\"/report\", report_file_name)\n",
    "        with open(report_file_path, 'w', encoding='utf-8') as report_file:\n",
    "            report_file.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004044dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
